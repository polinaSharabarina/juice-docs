"use strict";(self.webpackChunkjuice_docs=self.webpackChunkjuice_docs||[]).push([[3928],{3204:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"juice/pro-users/performance-latency/example","title":"Example","description":"Let\u2019s see how juice performs on a simple benchmark modeled after this NVIDIA blog post introducing the CUDA Graph API. The author takes a fairly simplistic workflow that consists of a single small kernel doing minimal work repeatedly applied to a buffer of data. The author starts with a naive implementation that performs a manual synchronization call after each kernel invocation, then refactors to an approach with synchronization calls only performed after a batch of kernels are executed. Finally, they introduce the graph API which allows for CUDA calls to be recorded and replayed with a single function invocation. These approaches, as well as another improvement are diagrammed in figure 1.","source":"@site/docs/juice/pro-users/performance-latency/example.md","sourceDirName":"juice/pro-users/performance-latency","slug":"/juice/pro-users/performance-latency/example","permalink":"/juice-docs/docs/juice/pro-users/performance-latency/example","draft":false,"unlisted":false,"editUrl":"https://github.com/juice-labs/juice-docs/edit/master/docs/juice/pro-users/performance-latency/example.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Example","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Examples","permalink":"/juice-docs/docs/juice/pro-users/running-applications/examples"},"next":{"title":"Example Workloads","permalink":"/juice-docs/docs/juice/pro-users/performance-latency/example-workloads"}}');var o=n(4848),s=n(8453);const i={title:"Example",sidebar_position:2},r="Example",l={},c=[];function h(e){const t={a:"a",admonition:"admonition",h1:"h1",header:"header",img:"img",p:"p",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"example",children:"Example"})}),"\n",(0,o.jsxs)(t.p,{children:["Let\u2019s see how juice performs on a simple benchmark modeled after this ",(0,o.jsx)(t.a,{href:"https://developer.nvidia.com/blog/cuda-graphs/",children:"NVIDIA blog post introducing the CUDA Graph API"}),". The author takes a fairly simplistic workflow that consists of a single small kernel doing minimal work repeatedly applied to a buffer of data. The author starts with a naive implementation that performs a manual synchronization call after each kernel invocation, then refactors to an approach with synchronization calls only performed after a batch of kernels are executed. Finally, they introduce the graph API which allows for CUDA calls to be recorded and replayed with a single function invocation. These approaches, as well as another improvement are diagrammed in figure 1."]}),"\n",(0,o.jsx)(t.p,{children:"When the simple kernel is run in a batch of 1000 executions repeated 1000 times, we see the following performance from the native execution vs with juice:"}),"\n",(0,o.jsxs)(t.table,{children:[(0,o.jsx)(t.thead,{children:(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.th,{style:{textAlign:"center"}}),(0,o.jsx)(t.th,{style:{textAlign:"center"},children:"Native [seconds]"}),(0,o.jsx)(t.th,{style:{textAlign:"center"},children:"juice [seconds]"}),(0,o.jsx)(t.th,{style:{textAlign:"center"},children:"Slowdown"})]})}),(0,o.jsxs)(t.tbody,{children:[(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"Naive implementation"}),(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"6.94"}),(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"54.93"}),(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"691%"})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"Reduced synchronizing"}),(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"4.06"}),(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"8.79"}),(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"116%"})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"Graph-based execution"}),(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"3.52"}),(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"3.70"}),(0,o.jsx)(t.td,{style:{textAlign:"center"},children:"5%"})]})]})]}),"\n",(0,o.jsx)(t.admonition,{type:"info",children:(0,o.jsx)(t.p,{children:"Naively implemented, the existence of cudaStreamSynchronize calls after each kernel launches. This means that each subsequent kernel is not launched until the previous one completes."})}),"\n",(0,o.jsx)(t.p,{children:"For us, cudaStreamSynchronize stalls until the state of the stream on the client matches the state on the server. Natively, CUDA does the same thing, however for juice, since the latency is variable and dependent on network performance, this stall can take a longer time to resolve. We need to wait for the device to flush and synchronize itself alongside whatever subtle latency is introduced by the chatter between server and client. E.g. on a 70ms latent connection, a naively placed cudaStreamSynchronize is effectively a 70ms sleep. If the GPU operation extends beyond this 70ms, the performance impact won\u2019t be noticeable. When the GPU is not sufficiently saturated this call will become a bottleneck."}),"\n",(0,o.jsx)(t.p,{children:"This overhead is neatly addressed (locally and remotely) through the graph API which takes those thousand kernel launches and wraps them into one invocation that allows juice to make fewer round trip calls to the server. The goal here is to fully saturate the GPU, always."}),"\n",(0,o.jsx)(t.p,{children:"Another means of hiding latency is to interleave multiple independent jobs. Assume that we need to complete eight independent tasks of the workload from the article and that we have sufficient memory to store all runs together. Rather than simply running our routine eight times in a row we can take advantage of CUDA events and queue up multiple batches of work at a time."}),"\n",(0,o.jsx)(t.p,{children:"A rough sketch of the algorithm would be to schedule a graph launch (cudaGraphLaunch) on a CUDA stream followed by recording an event (cudaEventRecord) for each of our tasks. Once enough work is queued onto a stream we can query the events and queue new batches as the old ones complete. The advantage of this system is that the GPU can continue to do meaningful work while the CPU manages task state. Extending our reference code from the previous example to interleave eight jobs (listing 2) we observe an execution time of 27.78 seconds natively and 28.16 seconds with juice. This corresponds to a slowdown of only 1.4%. Additionally, both runtimes complete in less time than it would take to simply run the workflow eight times. This concept could (and likely should) be extended to a dynamic pool that can dispatch enough work to saturate the GPU while keeping the memory footprint manageable."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Latency Example",src:n(4302).A+"",width:"610",height:"305"})})]})}function d(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},4302:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/latency_example-d3133d0687a17d80435d7032c05482ff.png"},8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>r});var a=n(6540);const o={},s=a.createContext(o);function i(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(s.Provider,{value:t},e.children)}}}]);